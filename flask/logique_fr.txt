Logique mathématique


La logique mathématique ou métamathématique est une discipline des mathématiques inventée à la fin du XIXe siècle, qui s'est donné comme objet l'étude des mathématiques en tant que langage.

Les objets fondamentaux de la logique mathématique sont les formules représentant les énoncés mathématiques, les dérivations ou démonstrations formelles représentant les raisonnements mathématiques et les sémantiques ou modèles ou interprétations dans des structures qui donnent un « sens » mathématique générique aux formules (et parfois même aux démonstrations) comme certains invariants : par exemple l'interprétation des formules du calcul des prédicats permet de leur affecter une valeur de vérité.
Histoire de la logique mathématique
Article détaillé : Histoire de la logique.
Naissance de la logique mathématique et logicisme
Article connexe : Logicisme.

La logique mathématique[2] est née à la fin du XIXe siècle de la logique au sens philosophique du terme ; elle est l'une des pistes explorées par les mathématiciens de cette époque afin de résoudre la crise des fondements mathématiques provoquée par la complexification des mathématiques et l'apparition des paradoxes. Ses débuts sont marqués par la rencontre entre deux idées nouvelles :

    la volonté chez Frege, Russell, Peano et Hilbert de donner une fondation axiomatique aux mathématiques ;
    la découverte par George Boole de l'existence de structures algébriques permettant de définir un « calcul de vérité ».

La logique mathématique se fonde sur les premières tentatives de traitement formel des mathématiques, dues à Leibniz et Lambert (fin XVIIe siècle - début XVIIIe siècle). Leibniz a en particulier introduit une grande partie de la notation mathématique moderne (usage des quantificateurs, symbole d'intégration, etc.). Toutefois on ne peut parler de logique mathématique qu'à partir du milieu du XIXe siècle, avec les travaux de George Boole (et dans une moindre mesure ceux d'Auguste De Morgan) qui introduit un calcul de vérité où les combinaisons logiques comme la conjonction, la disjonction et l'implication, sont des opérations analogues à l'addition ou la multiplication des entiers, mais portant sur les valeurs de vérité faux et vrai (ou 0 et 1) ; ces opérations booléennes se définissent au moyen de tables de vérité.

Le calcul de Boole introduisait l'idée, apparemment paradoxale mais spectaculairement fructueuse, que le langage mathématique pouvait se définir mathématiquement et devenir un objet d'étude pour les mathématiciens. Cependant, il ne permettait pas encore de résoudre les problèmes de fondements. Ainsi, de nombreux mathématiciens ont cherché à étendre ce concept au cadre général du raisonnement mathématique, ce qui a conduit à l'émergence des systèmes logiques formalisés. L'un des premiers de ces systèmes a été développé par Gottlob Frege au tournant du XXe siècle. Frege a posé les bases de la logique moderne, influençant profondément la philosophie des mathématiques et ouvrant la voie à des développements comme le lambda-calcul d'Alonzo Church et les machines de Turing.
Les années 1920 et David Hilbert

En 1900 au cours d'une très célèbre conférence au congrès international des mathématiciens à Paris, David Hilbert a proposé une liste des 23 problèmes non résolus les plus importants des mathématiques d'alors. Le deuxième était celui de la cohérence de l'arithmétique, c’est-à-dire de démontrer par des moyens finitistes la non-contradiction des axiomes de l'arithmétique.

Le programme de Hilbert a suscité de nombreux travaux en logique dans le premier quart du siècle, notamment le développement de systèmes d'axiomes pour les mathématiques : les axiomes de Peano pour l'arithmétique, ceux de Zermelo complétés par Skolem et Fraenkel pour la théorie des ensembles et le développement par Whitehead et Russell d'un programme de formalisation des mathématiques, les Principia Mathematica. C'est également la période où apparaissent les principes fondateurs de la théorie des modèles : notion de modèle d'une théorie, théorème de Löwenheim-Skolem.
La logique à partir des années 1930 et Kurt Gödel

En 1929 Kurt Gödel montre dans sa thèse de doctorat son théorème de complétude qui énonce le succès de l'entreprise de formalisation des mathématiques : tout raisonnement mathématique peut en principe être formalisé dans le calcul des prédicats. Ce théorème a été accueilli comme une avancée notable vers la résolution du programme de Hilbert, mais un an plus tard, Gödel démontrait le théorème d'incomplétude (publié en 1931) qui montrait irréfutablement l'impossibilité de réaliser ce programme.

Ce résultat négatif n'a toutefois pas arrêté l'essor de la logique mathématique. Les années 1930 ont vu arriver une nouvelle génération de logiciens anglais et américains, notamment Alonzo Church, Alan Turing, Stephen Kleene, Haskell Curry et Emil Post, qui ont grandement contribué à la définition de la notion d'algorithme et au développement de la théorie de la complexité algorithmique (théorie de la calculabilité, théorie de la complexité des algorithmes). La théorie de la démonstration de Hilbert a également continué à se développer avec les travaux de Gerhard Gentzen qui a produit la première démonstration de cohérence relative et initié ainsi un programme de classification des théories axiomatiques.

Le résultat le plus spectaculaire de l'après-guerre est dû à Paul Cohen qui démontre en utilisant la méthode du forcing l'indépendance de l'hypothèse du continu en théorie des ensembles, résolvant ainsi le 1er problème de Hilbert. Mais la logique mathématique subit également une révolution due à l'apparition de l'informatique ; la découverte de la correspondance de Curry-Howard, qui relie les preuves formelles au lambda-calcul de Church et donne un contenu calculatoire aux démonstrations, va déclencher un vaste programme de recherche.
Intérêt de la logique mathématique dans les mathématiques
Interactions entre la logique et les mathématiques

L'intérêt principal de la logique réside dans ses interactions avec d'autres domaines des mathématiques et les nouvelles méthodes qu'elle y apporte. De ce point de vue les réalisations les plus importantes viennent de la théorie des modèles qui est parfois considérée comme une branche de l'algèbre plutôt que de la logique ; la théorie des modèles s'applique notamment en théorie des groupes et en combinatoire (théorie de Ramsey).

D'autres interactions très productives existent toutefois : le développement de la théorie des ensembles est intimement lié à celui de la théorie de la mesure et a donné lieu à un domaine mathématique à part entière, la théorie descriptive des ensembles.[non pertinent] La théorie de la calculabilité est l'un des fondements de l'informatique théorique.

Depuis la fin du XXe siècle on a vu la théorie de la démonstration s'associer à la théorie des catégories et par ce biais commencer à interagir avec la topologie algébrique. D'autre part avec l'apparition de la logique linéaire elle entretient également des liens de plus en plus étroit avec l'algèbre linéaire, voire avec la géométrie non commutative. Plus récemment la théorie homotopique des types crée une connexion riche entre la logique (la théorie des types) et les mathématiques (la théorie de l'homotopie) dont on n'entrevoit que les prémices.
Formalisation

La formalisation des mathématiques dans des systèmes logiques, qui a suscité en particulier les travaux de Whitehead et Russell, a été l'une des grandes motivation du développement de la logique mathématique. L'apparition d'outils informatiques spécialisés, démonstrateurs automatiques, systèmes experts et assistants de preuve, a donné un nouvel intérêt à ce programme. Les assistants de preuve en particulier ont plusieurs applications en mathématique.

Tout d'abord dans la fin du XXe siècle et au début du XXIe siècle deux anciennes conjectures ont été résolues en faisant appel à l'ordinateur pour traiter un très grand nombre de cas : le théorème des quatre couleurs et la conjecture de Kepler. Les doutes soulevés par cette utilisation de l'ordinateur ont motivé la formalisation et la vérification complète de ces démonstrations.

D'autre part des programmes de formalisation de mathématiques utilisant les assistants de preuves se sont développés afin de produire un corpus complètement formalisé de mathématiques ; pour les mathématiques l'existence d'un tel corpus aurait en particulier l'intérêt de permettre des traitements algorithmiques comme la recherche par motif : trouver tous les théorèmes qui se déduisent du théorème des nombres premiers, trouver tous les théorèmes à propos de la fonction zeta de Riemann, etc.
Quelques résultats fondamentaux

Quelques résultats importants ont été établis pendant la décennie 1930.

Le théorème de complétude du calcul des prédicats du premier ordre que Gödel a montré dans sa thèse de doctorat, un an avant son célèbre théorème d'incomplétude. Ce théorème énonce que toute démonstration mathématique peut être représentée dans le formalisme du calcul des prédicats (qui est donc complet).

L'ensemble des théorèmes du calcul des prédicats n'est pas calculable, c'est-à-dire qu'aucun algorithme ne permet de vérifier si un énoncé donné est prouvable ou non. Il existe cependant un algorithme qui étant donné une formule du premier ordre en trouve une preuve en un temps fini s'il en existe une, mais continue indéfiniment sinon. On dit que l'ensemble des formules du premier ordre prouvables est « récursivement énumérable » ou « semi-décidable ».

La cohérence (non-contradiction) d'une théorie (ensemble d'axiomes), permettant de formaliser (au moins) l'arithmétique (par exemple les axiomes de Peano) n'est pas une conséquence de ces seuls axiomes[3]. C'est le fameux second théorème d'incomplétude de Gödel.

Tout théorème purement logique peut être démontré en n'utilisant que des propositions qui sont des sous-formules de son énoncé. Connu sous le nom de propriété de la sous-formule, ce résultat est une conséquence du théorème d'élimination des coupures en calcul des séquents de Gerhard Gentzen : la propriété de la sous-formule a pour conséquence la cohérence de la logique, car elle interdit la dérivation de la formule vide (identifiée à l'absurdité).

D'autres résultats importants ont été établis pendant la deuxième partie du XXe siècle. L'indépendance de l'hypothèse du continu par rapport aux autres axiomes de la théorie des ensembles (ZF) est achevée en 1963 par Paul Cohen[4]. La théorie de la calculabilité se développe. Au tournant de la décennie 1980, la correspondance de Curry-Howard identifie la simplification des démonstrations et les programmes, créant ainsi un pont entre mathématiques et informatique. En 1990, cette correspondance est étendue à la logique classique. La mécanisation, et donc la formalisation, complète de théorème de mathématiques comme le théorème des quatre couleurs ou le théorème de Feit-Thompson.

Au XXIe siècle, émergent de nouvelles branches prometteuses comme la théorie homotopique des types.
Système logique
Définition

Un système logique (ou une logique) est un système formel auquel on ajoute une sémantique[5]. Un système formel se constitue :

    d'une syntaxe (ou langage) dont on lui attribue deux données : un ensemble de symboles servant à la construction de formules. Dans les systèmes de logique classique ou intuitionniste, les formules représentent des énoncés mathématiques exprimés formellement. La deuxième donnée est un ensemble de règles de constructions de formules. Les formules sont définies par des moyens combinatoires à l'aide de ces règles : suites de symboles, arbres étiquetés, graphes…

    d'un système de déduction (ou d'un système d'inférence, ou encore d'un calcul) disposant d'un ensemble d'axiomes et de règles d'inférence. Les déductions sont également définies par des moyens combinatoires. Une déduction permet de dériver des formules (les formules prouvables ou théorèmes) à partir de formules de départ (les axiomes) au moyen de règles (les règles d'inférence).

La sémantique sert à attribuer à chaque formule construite à partir d'un système formel un sens, voire une valeur de vérité. Cette attribution se fait au moyen d'une interprétation (ou valuation) des formules; il s'agit d'une fonction associant à toute formule un objet dans une structure abstraite appelée modèle, ce qui permet de définir la validité des formules. Elle permet ainsi d'interpréter les formules d'un système formel dans un contexte donné. Dans le cadre de la logique classique, il s'agit d'attribuer à chaque formule la valeur Vrai ou la valeur Faux, qu'on peut même respectivement identifier à donner la valeur 1 ou la valeur 0 (voir Algèbre de Boole).

Il arrive parfois que l'on puisse définir la syntaxe comme étant le système de déduction, et qu'on appelle calcul le système formel entier, voire le système logique entier. De ce fait, il est courant d'utiliser le nom de calcul des propositions pour désigner ce qu'on devrait appeler logique des propositions, de la même façon qu'on peut confondre calcul des prédicats et logique des prédicats.
Syntaxe et sémantique

La caractéristique principale des formules et des déductions est qu'il s'agit d'objets finis ; plus encore, chacun des ensembles de formules et de déductions est récursif, c'est-à-dire que l'on dispose d'un algorithme qui détermine si un objet donné est une formule correcte ou une déduction correcte du système. L'étude de logique du point de vue des formules et des expressions s'appelle la syntaxe.

La sémantique, au contraire, échappe à la combinatoire en faisant appel (en général) à des objets infinis. Elle a d'abord servi à « définir » la vérité. Par exemple, en calcul des propositions, les formules sont interprétées, par exemple, par des éléments d'une algèbre de Boole.

Une mise en garde est de rigueur ici, car Kurt Gödel (suivi par Alfred Tarski) a montré avec son théorème d'incomplétude l'impossibilité de justifier mathématiquement la rigueur mathématique dans les mathématiques. C'est pourquoi il est plus approprié de voir la sémantique comme une métaphore : les formules — objets syntaxiques — sont projetées dans un autre monde. Cette technique — plonger les objets dans un monde plus vaste pour raisonner sur eux — est couramment utilisée en mathématique et est efficace.

Cependant, la sémantique ne sert pas qu'à « définir » la vérité. Par exemple, la sémantique dénotationnelle est une interprétation, non des formules, mais des déductions visant à capturer leur contenu calculatoire.
Systèmes de déduction

En logiques classique et intuitionniste, on distingue deux types d'axiomes : les axiomes logiques qui expriment des propriétés purement logiques comme A ∨ ¬ A {\displaystyle A\lor \lnot A} (principe du tiers exclu, valide en logique classique mais pas en logique intuitionniste) et les axiomes extra-logiques qui définissent des objets mathématiques ; par exemple les axiomes de Peano définissent les entiers de l'arithmétique tandis que les axiomes de Zermelo-Fraenkel définissent les ensembles. Quand le système possède des axiomes extra-logiques, on parle de théorie axiomatique. L'étude des différents modèles d'une même théorie axiomatique est l'objet de la théorie des modèles.

Deux systèmes de déduction peuvent être équivalents au sens où ils ont exactement les mêmes théorèmes. On démontre cette équivalence en fournissant des traductions des déductions de l'un dans les déductions de l'autre. Par exemple, il existe (au moins) trois types de systèmes de déduction équivalents pour le calcul des prédicats : les systèmes à la Hilbert, la déduction naturelle et le calcul des séquents. On y ajoute parfois le style de Fitch qui est une variante de la déduction naturelle dans laquelle les démonstrations sont présentées de façon linéaire.

Alors que la théorie des nombres démontre des propriétés des nombres, on notera la principale caractéristique de la logique en tant que théorie mathématique : elle « démontre » des propriétés de systèmes de déduction dans lesquels les objets sont des « théorèmes ». Il y a donc un double niveau dans lequel il ne faut pas se perdre. Pour clarifier les choses, les « théorèmes » énonçant des propriétés des systèmes de déduction sont parfois appelés des « métathéorèmes ». Le système de déduction que l'on étudie est appelé la « théorie » et le système dans lequel on énonce et démontre les métathéorèmes est appelé la « métathéorie ».

Les propriétés fondamentales des systèmes de déduction sont la correction, la cohérence, la complétude .

La correction énonce que les théorèmes sont valides dans tous les modèles. Cela signifie que les règles d'inférence sont bien adaptées à ces modèles, autrement dit que le système de déduction formalise correctement le raisonnement dans ces modèles.

Un système de déduction est cohérent (on emploie aussi l'anglicisme consistant) s'il admet un modèle, ou ce qui revient au même, s'il ne permet pas de démontrer n'importe quelle formule. On parle aussi de cohérence équationnelle lorsque le système admet un modèle non trivial, c'est-à-dire ayant au moins deux éléments. Cela revient à affirmer que le système ne permet pas de démontrer n'importe quelle équation.

La complétude se définit par rapport à une famille de modèles. Un système de déduction est complet par rapport à une famille de modèles, si toute proposition qui est valide dans tous les modèles de la famille est un théorème. En bref, un système est complet si tout ce qui est valide est démontrable.

Une autre propriété des systèmes de déduction apparentée à la complétude est la cohérence maximale. Un système de déduction est maximalement cohérent, si l'ajout d'un nouvel axiome qui n'est pas lui-même un théorème rend le système incohérent.

Affirmer « tel système est complet pour telle famille de modèles » est un exemple typique de métathéorème.

Dans ce cadre, la notion intuitive de vérité donne lieu à deux concepts formels : la validité et la démontrabilité. Les trois propriétés de correction, cohérence et complétude précisent comment ces notions peuvent être reliées, espérant qu'ainsi la vérité, quête du logicien, puisse être cernée.
Le calcul des propositions
Article détaillé : Calcul des propositions.

Les propositions sont des formules exprimant des faits mathématiques, c'est-à-dire des propriétés qui ne dépendent d'aucun paramètre, et qui donc ne peuvent qu'être vraies ou fausses[6], comme « 3 est un multiple de 4 » (au contraire de « n est un multiple de 4 », qui est vraie ou fausse selon la valeur que l'on donne au paramètre n) ou « les zéros non triviaux de la fonction zêta de Riemann ont tous pour partie réelle 1/2 ».

Les propositions élémentaires, appelées variables propositionnelles, sont combinées au moyen de connecteurs pour former des formules complexes. Les propositions peuvent être interprétées en remplaçant chaque variable propositionnelle par une proposition. Par exemple une interprétation de la proposition ( P ∧ ¬ P ) ⟹ Q {\displaystyle (P\wedge \neg P)\implies Q} pourrait être « si 3 est pair et 3 est impair alors 0 = 1 ».
Connecteurs les plus fréquents

Les connecteurs sont présentés avec leur interprétation en logique classique.

La disjonction de deux propositions P et Q est la proposition notée P ∨ Q {\displaystyle P\vee Q} ou « P ou Q » qui est vraie si au moins une des deux propositions est vraie; elle est donc fausse uniquement si les deux propositions sont fausses.

La négation d’une proposition P est la proposition notée ¬P, ou « non P » qui est vraie lorsque P est fausse; elle est donc fausse lorsque P est vraie.

À partir de ces deux connecteurs, on peut construire d’autres connecteurs ou abréviations utiles.

La conjonction de deux propositions P et Q est la proposition suivante :

    ¬ ( ( ¬ P ) ∨ ( ¬ Q ) ) {\displaystyle \neg ((\neg P)\vee (\neg Q))} c'est-à-dire non ( (non P) ou (non Q) )

Celle-ci est notée P ∧ Q ou « P et Q » et n’est vraie que lorsque P et Q sont vraies et est donc fausse si l’une des deux propositions est fausse.

L'implication de Q par P est la proposition ( ¬ P ) ∨ Q {\displaystyle (\neg P)\vee Q}, notée «  P ⟹ Q {\displaystyle P\implies Q} » ou « P implique Q », et qui est fausse seulement si P est une proposition vraie et Q fausse.

L'équivalence logique de P et Q est la proposition ( ( P ⟹ Q ) ∧ ( Q ⟹ P ) ) {\displaystyle {\bigg (}(P\implies Q)\wedge (Q\implies P){\bigg )}} ( ((P implique Q) et (Q implique P) )), notée «  P ⟺ Q {\displaystyle P\iff Q} » ou (P est équivalent à Q), et qui n'est vraie que si les deux propositions P et Q ont même valeur de vérité.

Le ou exclusif ou disjonction exclusive de P et Q est la proposition P ∨ _ Q {\displaystyle P\;{\underline {\vee }}\;Q} (parfois aussi notée[7] P ⊕ Q ou encore[8] P | Q ou par les concepteurs de circuits signe XOR) qui correspond à ( P ∨ Q ) ∧ ¬ ( P ∧ Q ) {\displaystyle (P\vee Q)\wedge \neg (P\wedge Q)}, c'est-à-dire en français : soit P, soit Q, mais pas les deux à la fois. Le ou exclusif de P et Q correspond à P ⇔ ¬Q ou encore à ¬(P ⇔ Q). Cette proposition n'est vraie que si P et Q ont des valeurs de vérités distinctes.
Connecteur universel

Une caractéristique du calcul propositionnel dit « classique » est que toutes les propositions peuvent s'exprimer à partir de deux connecteurs : par exemple ∨ et ¬ (ou et non)[9]. Mais d'autres choix sont possibles comme ⇒ (implication) et ⊥ (faux). On peut n'utiliser qu'un seul connecteur, le symbole de Sheffer « | » (Henry M. Sheffer, 1913), appelé « stroke » par son concepteur et appelé aujourd'hui Nand et noté signe NAND par les concepteurs de circuits ; on peut aussi n'utiliser que le connecteur Nor (noté signe NOR) comme l'a remarqué Charles Sanders Peirce (1880) sans le publier. En somme, en logique classique, un unique connecteur suffit pour rendre compte de toutes les opérations logiques.
Le calcul des prédicats
Articles détaillés : Calcul des prédicats et Logique d'ordre supérieur.

Le calcul des prédicats étend le calcul propositionnel en permettant d'écrire des formules qui dépendent de paramètres ; pour cela le calcul des prédicats introduit les notions de variables, de symboles de fonctions et de relations, de termes et de quantificateurs ; les termes sont obtenus en combinant les variables au moyen des symboles de fonction, les formules élémentaires sont obtenues en appliquant les symboles de relations à des termes.

Une formule typique du calcul des prédicats est ∀ a , b ( ( p = a × b ) ⟹ ( a = 1 ) ∨ ( b = 1 ) ) {\displaystyle \forall a,b\;{\bigg (}(p=a\times b)\implies (a=1)\vee (b=1){\bigg )}} qui lorsqu'on l'interprète dans les entiers exprime que le paramètre p est un nombre premier (ou 1). Cette formule utilise deux symboles de fonction (le point, fonction binaire interprétée par la multiplication des entiers, et le symbole « 1 », fonction 0-aire, c'est-à-dire constante) et un symbole de relation (pour l'égalité). Les variables sont a, b et p, les termes sont a.b et 1 et les formules élémentaires sont « p = a.b », « a = 1 » et « b = 1 ». Les variables a et b sont quantifiées mais pas la variable p dont la formule dépend.

Il existe plusieurs variantes du calcul des prédicats ; dans la plus simple, le calcul des prédicats du premier ordre, les variables représentent toutes les mêmes types d'objets, par exemple dans la formule ci-dessus, les 3 variables a, b et p vont toutes représenter des entiers. Dans le calcul des prédicats du second ordre, il y a deux types de variables : des variables pour les objets et d'autres pour les prédicats, c'est-à-dire les relations entre objets. Par exemple en arithmétique du second ordre on emploie des variables pour représenter des entiers, et d'autres pour des ensembles d'entiers. La hiérarchie continue ainsi, au 3e ordre on a 3 types de variables pour les objets, les relations entre objets, les relations entre relations, etc.
Substitution

Pour décrire le calcul des prédicats, une opération essentielle est la substitution qui consiste à remplacer dans une formule toutes les occurrences d'une variable x par un terme a, obtenant ainsi une nouvelle formule ; par exemple si on remplace la variable p par le terme n! + 1 dans la formule ci-dessus on obtient la formule «  ∀ a ∀ b ( ( n ! + 1 = a ⋅ b ) ⟹ ( ( a = 1 ) ∨ ( b = 1 ) ) {\displaystyle \forall a\forall b((n!+1=a\cdot b)\implies ((a=1)\vee (b=1))} » (qui exprime que la factorielle de l'entier n plus 1 est un nombre premier).

Si P est une formule dépendant d'un paramètre x et a est un terme, l’assemblage obtenu en remplaçant x par a dans P est une formule qui peut se noter P [ a / x ] {\displaystyle P[a/x]} ou ( a ∣ x ) P {\displaystyle (a\mid x)P}, ou d'autres variantes de ces notations. et s’appelle formule obtenue par substitution de x par a dans P.

Pour mettre en évidence un paramètre x dont dépend une formule P, on écrit celle-ci sous la forme P{x} ; on note alors P{a} la proposition (a|x)P.

On peut chercher à trouver la (les) substitution(s) qui rend(ent) une formule prouvable ; le problème est particulièrement intéressant dans le cas de formules dites équationnelles, c'est-à-dire de la forme t(x) = t'(x). La théorie qui cherche à résoudre de telles équations dans le cadre de la logique mathématique s'appelle l'unification.
Les quantificateurs

Les quantificateurs sont les ingrédients syntaxiques spécifiques du calcul des prédicats. Comme les connecteurs propositionnels ils permettent de construire de nouvelles formules à partir d'anciennes, mais ils s'appuient pour cela sur l'utilisation des variables.
Article détaillé : Quantificateurs.

Soit une formule du calcul des prédicats P. On construit alors une nouvelle formule dite existentielle notée ∃ x P qui se lit « il existe x tel que P ». Supposons que P ne « dépende » que de x. La proposition ∃ x P est vraie quand il existe au moins un objet a (dans le domaine considéré, celui sur lequel « varie » x) tel que, quand on substitue a à x dans P on obtienne une proposition vraie. La formule P est vue comme une propriété, et ∃ x P est vraie quand il existe un objet ayant cette propriété.

Le signe ∃ s’appelle le quantificateur existentiel.

De même on peut construire à partir de P une formule dite universelle notée ∀ x P, qui se lit « pour tout x P » ou quel que soit x P. Elle signifie que tous les objets du domaine considéré (ceux que x est susceptible de désigner) possèdent la propriété décrite par P.

Le signe ∀ s’appelle le quantificateur universel.

En logique classique les quantificateurs universel et existentiel peuvent se définir l'un par rapport à l'autre, par négation car
∀ x P {\displaystyle \forall x\;P} équivaut à ¬ ∃ x ¬ P {\displaystyle \neg \exists x\;\neg P}   et   ∃ x P {\displaystyle \exists x\;P} équivaut à ¬ ∀ x ¬ P {\displaystyle \neg \forall x\;\neg P}

En effet « il est faux que tout objet possède une propriété donnée » signifie « il en existe au moins un qui ne possède pas cette propriété ».
Utilisation des quantificateurs
Propriétés élémentaires

Soient P et Q deux propositions et x un objet indéterminé.

    ¬ ( ∃ x P ) ⇔ ( ∀ x ¬ P ) {\displaystyle \neg (\exists x\;P)\Leftrightarrow (\forall x\;\neg P)}
    ( ∀ x ) ( P ∧ Q ) ⇔ ( ( ∀ x ) P ∧ ( ∀ x ) Q ) {\displaystyle (\forall x)(P\wedge Q)\Leftrightarrow ((\forall x)P\wedge (\forall x)Q)}
    ( ∀ x ) P ∨ ( ∀ x ) Q ⇒ ( ∀ x ) ( P ∨ Q ) {\displaystyle (\forall x)P\vee (\forall x)Q\Rightarrow (\forall x)(P\vee Q)}

(Implication réciproque fausse en général)

    ( ∃ x ) ( P ∨ Q ) ⇔ ( ( ∃ x ) P ∨ ( ∃ x ) Q ) {\displaystyle (\exists x)(P\vee Q)\Leftrightarrow ((\exists x)P\vee (\exists x)Q)}
    ( ∃ x ) ( P ∧ Q ) ⇒ ( ( ∃ x ) P ∧ ( ∃ x ) Q ) {\displaystyle (\exists x)(P\wedge Q)\Rightarrow ((\exists x)P\wedge (\exists x)Q)}

(Implication réciproque fausse en général)
Propriétés utiles

Soient P une proposition et x et y des objets indéterminés.

    ( ∀ x ) ( ∀ y ) P ⇔ ( ∀ y ) ( ∀ x ) P {\displaystyle (\forall x)(\forall y)P\Leftrightarrow (\forall y)(\forall x)P}
    ( ∃ x ) ( ∃ y ) P ⇔ ( ∃ y ) ( ∃ x ) P {\displaystyle (\exists x)(\exists y)P\Leftrightarrow (\exists y)(\exists x)P}
    ( ∃ x ) ( ∀ y ) P ⇒ ( ∀ y ) ( ∃ x ) P {\displaystyle (\exists x)(\forall y)P\Rightarrow (\forall y)(\exists x)P}

S’il existe un x, tel que pour tout y, on ait P vraie, alors pour tout y, il existe bien un x (celui obtenu avant) tel que P soit vraie.
L’implication réciproque est fausse en général, parce que si pour chaque y, il existe un x tel que P soit vraie, ce x pourrait dépendre de y et varier suivant y. Ce x pourrait donc ne pas être le même pour tout y tel que P soit vraie.
Propriétés moins intuitives

    ∀ x P x ⟹ ∃ x P x {\displaystyle \forall x\;Px\implies \exists xPx}[10]

Si A est une formule où la variable x n'apparaît pas librement, on a[11] :

    ( A ⟹ ∀ x P x ) ⇔ ∀ x ( A ⟹ P x ) {\displaystyle (A\implies \forall xPx)\Leftrightarrow \forall x(A\implies Px)}
    ( A ⟹ ∃ x P x ) ⇔ ∃ x ( A ⟹ P x ) {\displaystyle (A\implies \exists xPx)\Leftrightarrow \exists x(A\implies Px)}

Mais aussi, ce qui est moins intuitif :

    ( ∀ x P x ⟹ A ) ⇔ ∃ x ( P x ⟹ A ) {\displaystyle (\forall xPx\implies A)\Leftrightarrow \exists x(Px\implies A)}
    ( ∃ x P x ⟹ A ) ⇔ ∀ x ( P x ⟹ A ) {\displaystyle (\exists xPx\implies A)\Leftrightarrow \forall x(Px\implies A)}

Associé à cette dernière règle, voir le paradoxe du buveur.
L’égalité

Le symbole de relation « = » qui représente l'égalité, a un statut un petit peu particulier, à la frontière entre les symboles logiques (connecteurs, quantificateurs) et les symboles non logiques (relations, fonctions). La formule a = b signifie que a et b représentent le même objet (ou encore que a et b sont des notations différentes du même objet), et se lit « a est égal à b ».

La théorie des modèles classique se développe le plus souvent dans le cadre du calcul des prédicats avec égalité, c'est-à-dire que les théories considérées sont égalitaires : la relation d'égalité est utilisée en plus des symboles de la signature de la théorie.

Du point de vue de la déduction, l'égalité est régie par des axiomes, décrits ci-dessous, exprimant essentiellement que deux objets égaux ont les mêmes propriétés (et, en logique du second ordre, que la réciproque est vraie).

La négation de « = » est la relation « ≠ » qui est définie par a ≠ b si et seulement si ¬(a = b).
Axiomes de l'égalité en logique du premier ordre

En logique du premier ordre :

    ∀ x ( x = x ) {\displaystyle \forall x\;(x=x)} (réflexivité de =)
    ∀ x ( ∀ y ) ( ( x = y ) ⇔ ( y = x ) {\displaystyle \forall x(\forall y)((x=y)\Leftrightarrow (y=x)} (symétrie de =)
    ∀ x ( ∀ y ) ( ∀ z ) ( ( ( x = y ) ∧ ( y = z ) ) ⟹ ( x = z ) ) {\displaystyle \forall x(\forall y)(\forall z)(((x=y)\wedge (y=z))\implies (x=z))} (transitivité de =)

La relation = étant réflexive, symétrique et transitive, on dit que la relation = est une relation d'équivalence.

    Soit P{x} une formule dépendant d'une variable x. Soient a et b deux termes tels que a = b. Alors les propositions P{a} et P{b} sont équivalentes. Ces axiomes (un par formule P{x}) expriment que deux objets égaux ont les mêmes propriétés.
    Soit F(x) une fonction contenant une variable libre x. Soient a et b des termes tels que a = b. Alors les termes F(a) et F(b) (obtenus en remplaçant respectivement x par a et x par b dans F(x)) sont égaux.

Ces deux dernières propriétés expriment intuitivement que = est la plus fine des relations d'équivalence.
Définition en logique du second ordre

En logique du second ordre on peut définir plus simplement l'égalité par :

    a = b si et seulement si ∀P (Pa ⇔ Pb)

Autrement dit deux objets sont égaux si et seulement s'ils ont les mêmes propriétés (principe d'identité des indiscernables énoncé par Leibniz) 